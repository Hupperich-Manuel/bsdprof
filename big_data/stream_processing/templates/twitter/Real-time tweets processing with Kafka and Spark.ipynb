{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb94174",
   "metadata": {},
   "source": [
    "<img width=\"200\" style=\"float:left\" \n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf9ef9",
   "metadata": {},
   "source": [
    "<img style=\"float:left;\" \n",
    "     src=\"https://secemu.org/wp-content/uploads/2016/02/Twitter-Banner-1024x385.png\" />   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee7dc66",
   "metadata": {},
   "source": [
    "# Sections\n",
    "* [Description](#0)\n",
    "* [1. Setup](#1) \n",
    "  * [1.1 Start the Kafka service and the producer](#1.1)\n",
    "  * [1.2 Search for Spark Installation](#1.2)\n",
    "  * [1.3 Create SparkSession](#1.3)\n",
    "* [2. Use case](#2) \n",
    "  * [2.1 Main DataFrame creation](#2.1)\n",
    "  * [2.2 Map the sequence of bytes to a proper JSON document and display contents](#2.2)\n",
    "* [3. Finalize the exercise](#3)\n",
    "  * [3.1 Stop the Spark Streaming application](#3.1)\n",
    "  * [3.2 Stop the Kafka producer](#3.2)\n",
    "  * [3.3 Stop the Kafka service](#3.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b7c91",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "## Description\n",
    "<p>\n",
    "<div>This notebook will help you address the following:</div>\n",
    "<ul>    \n",
    "    <li>Consume events from a Kafka topic called <em>tweets</em></li>\n",
    "    <li>Translate tweets ingested in raw format (sequence of bytes) into the proper JSON format</li>\n",
    "    <li>See how to manipulate tweets - use your imagination here to implement a use case you like</li>\n",
    "    <li>Display the results of the manipulation on the consule; continue the journey of the real-time solution by yourself (ex. publish results in Kafka, MariaDB, MongoDB, ...)</li>\n",
    "</ul>    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81aa63",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331e2ce",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### 1.1 Start the Kafka service and the producer \n",
    "<p>Before you can run this notebook, be sure that you log into the course environment and:</p>\n",
    "<ul>\n",
    "    <li><p><b>Start the Kafka service</b>:\n",
    "        <br/><em>\\$ sudo service kafka start</p></em></li>\n",
    "    <li>Add your <b>API key</b>, <b>API secret</b>, <b>access token</b> and <b>access secret</b> to the <em>credentials.ini</em> file</li>\n",
    "    <li><p><b>Start the producer</b> connecting to Twitter and filtering tweets by keywords or hashtags:\n",
    "        <br/><em>\\$ python3 twitter_producer.py credentials.ini \"btc,#eth,ada\" -b localhost:9092 -t tweets</em></p></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff256c43",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.2 Search for Spark Installation \n",
    "This step is required just because we are working in the course environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69246e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82ec90",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### 1.3 Create SparkSession\n",
    "\n",
    "In addition to create the Spark Session, we're going to set up a variable environment to include extra libraries in our \"cluster\".<br/>\n",
    "In this case we're including the Spark package as our job will connect to Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26503986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.3\" pyspark-shell'\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Twitter\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d68f9",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee50d87",
   "metadata": {},
   "source": [
    "<a id='2.1'></a>\n",
    "### 2.1 Main DataFrame creation\n",
    "<p>Have a look at the schema you get by default when you create a DataFrame on top of a Kafka topic.<br/> The <b><em>value field</b></em> is the one containing the data from the <em>ingestion layer</em>, the Twitter producer in our case.</p>\n",
    "<p>Bear in mind thataAs <em>we're simplifying things</em>, we're not relying on schemas and <b>we're sending sequence of bytes to Kafka topics</b>.</p>\n",
    "<p>Later in the notebook, we're going to convert that sequence of bytes in a proper JSON document representing every tweet as it was received.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be323cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawTweetsDF = spark.readStream \\\n",
    "                   .format(\"kafka\") \\\n",
    "                   .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "                   .option(\"subscribe\", \"tweets\") \\\n",
    "                   .option(\"startingOffsets\", \"latest\") \\\n",
    "                   .load()\n",
    "rawTweetsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb2dd8",
   "metadata": {},
   "source": [
    "<a id='2.2'></a>\n",
    "### 2.2 Map the sequence of bytes to a proper JSON document and display contents\n",
    "We're going to apply the following logic to the events we get from the topic:\n",
    "<ol>\n",
    "    <li>Define the schema that matches the raw sequence of bytes we get from the topic.</li>\n",
    "    <li>Cast the default data type of the field <em>value</em> (byte) to the String data type.</li>\n",
    "    <li>Convert the String into a proper JSON document by using the <em>from_json</em> function.</li>\n",
    "    <li>Flatten the JSON file and display event time, user name, text and the id.</li>\n",
    "    <li>Display the results in the console.</li>\n",
    "</ol>\n",
    "\n",
    "Watch the schema we get now, it looks like any other DataFrame we've seen up until now... this is <b>a real unified processing framework</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7001996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Schema of a tweet coming from Twitter.\n",
    "\n",
    "tweet_schema=\"\"\"\n",
    "created_at string,\n",
    "id bigint,\n",
    "id_str string,\n",
    "text string,\n",
    "source string,\n",
    "truncated boolean,\n",
    "in_reply_to_status_id bigint,\n",
    "in_reply_to_status_id_str string,\n",
    "in_reply_to_user_id bigint,\n",
    "in_reply_to_user_id_str string,\n",
    "in_reply_to_screen_name string,\n",
    "`user` struct<\n",
    "            id:bigint,\n",
    "            id_str:string,\n",
    "            name:string,\n",
    "            screen_name:string,\n",
    "            location:string,\n",
    "            url:string,\n",
    "            description:string,\n",
    "            protected:boolean,\n",
    "            verified:boolean,\n",
    "            followers_count:bigint,\n",
    "            friends_count:bigint,\n",
    "            listed_count:bigint,\n",
    "            favourites_count:bigint,\n",
    "            statuses_count:bigint,\n",
    "            created_at:string,\n",
    "            profile_banner_url:string,\n",
    "            profile_image_url_https:string,\n",
    "            default_profile:boolean,\n",
    "            default_profile_image:boolean,\n",
    "            withheld_in_countries: array<string>,\n",
    "            withheld_scope:string,\n",
    "            geo_enabled:boolean\n",
    "            >,\n",
    "coordinates struct <\n",
    "            coordinates:array<float>,\n",
    "            type:string\n",
    "            >,\n",
    "place struct<\n",
    "            country:string,\n",
    "            country_code:string,\n",
    "            full_name:string,\n",
    "            place_type:string,\n",
    "            url:string\n",
    "            >,\n",
    "quoted_status_id bigint,\n",
    "quoted_status_id_str string,\n",
    "is_quote_status boolean,\n",
    "quote_count bigint,\n",
    "reply_count bigint,\n",
    "retweet_count bigint,\n",
    "favorite_count bigint,\n",
    "entities struct<\n",
    "            user_mentions:array<struct<screen_name:string>>,\n",
    "            hashtags:array<struct<text:string>>, \n",
    "            media:array<struct<expanded_url:string>>, \n",
    "            urls:array<struct<expanded_url:string>>, \n",
    "            symbols:array<struct<text:string>>\n",
    "            >,\n",
    "favorited boolean,\n",
    "retweeted boolean,\n",
    "possibly_sensitive boolean,\n",
    "filter_level string,\n",
    "lang string\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "# 2. Cast the default data type of the field value (byte) to the String data type.\n",
    "# 3. Convert the String into a proper JSON document by using the from_json function.\n",
    "# 4. Flatten the JSON file and display event time, user name, text and the id.\n",
    "\n",
    "tweetsDF = rawTweetsDF.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "                      .select(from_json(col(\"value\"), tweet_schema).alias(\"data\")) \\\n",
    "                      .select(col(\"data.created_at\").alias(\"event_time\"), \n",
    "                              col(\"data.user.screen_name\"),\n",
    "                              col(\"data.text\"),\n",
    "                              col(\"data.id\"))\n",
    "tweetsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Display the results in the console.\n",
    "\n",
    "tweetsDF.writeStream \\\n",
    "        .format(\"console\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .start() \\\n",
    "        .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5619c6e",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3. Finalize the exercise\n",
    "\n",
    "It's always good to terminate things properly to avoid any kind of corruption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a3e5bd",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "### 3.1 Stop the Spark Streaming application\n",
    "In order to stop the Spark Streaming application go to **Kernel -> Shutdown**, that's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912ddeae",
   "metadata": {},
   "source": [
    "<a id='3.2'></a>\n",
    "### 3.2 Stop the Kafka producer\n",
    "Go to the terminal where you started the producer and **press Ctrl + C**, it's that easy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9394c0fe",
   "metadata": {},
   "source": [
    "<a id='3.3'></a>\n",
    "### 3.3 Stop the Kafka service\n",
    "<p>Go to a terminal windows and type the following command:</p>\n",
    "<em>$ sudo service kafka stop</em>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
